---
title: Projekt BoardBase
subtitle: Sprawozdanie z Etapu 2 - Zaawansowane systemy baz danych
format:
  pdf:
    output-file: Etap 2 Sprawozdanie Krzysztof Dąbrowski 293101.pdf
    # fig-pos: 'H'
---

\listoflistings

{{< include ../shared/repo_callout.qmd >}}

# Funkcje i procedury dodawania elementów

## Funkcja
Przeczytałem, że liczenie arytmetycznej średniej ocen jest mało miarodajne dla małej liczby ocen, a użycie średniej bayesowskiej jest lepszym rozwiązaniem @algolia:bayesian-average. Napisałem \acr{UDF} liczącą w ten sposób średnią ocenę gry.

Dla gry z 5 pozytywnymi ocenami średnia arytmetyczna wychodziła 8.8, a wynik mojej \acr{UDF} [`fair_game_rating`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/programmable/user-defined-functions.sql) wyniósł 8.48(6).

## Procedury dodawania elementów

Utworzyłem kilka procedur do dodawania nowych rekordów do bazy danych w pliku [`create-records-procedures.sql`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/programmable/create-records-procedures.sql).

- `add_user` dodaje nowego użytkownika,
- `add_location` dodaje nowe miejsce do grania,
- `add_rating` dodaje nową ocenę gry,
- `add_review` dodaje nową recenzję,
- `add_game_wish` dodaje nową grę do listy życzeń.

Procedura `add_user` automatycznie wylicza hash hasła i przycina białe znaki z nazwy użytkownika i adresu email. \
Pozostałe procedury ułatwiają ustawiają automatycznie id użytkownika na pasujące do aktualnie zalogowanego użytkownika do bazy za pomocą `WITH` z \acr{CTE}.

## Dokumentacja działania

Przy testowaniu procedur miałem kłopot z deklarowaniem zmiennych. Jestem przyzwyczajony do deklarowania zmiennych w ramach sesji \acr{SQL} za pomocą zwykłego `DECLARE` \@nazwa zmiennej. \
Przeczytałem, że w PostgreSQL trzeba deklarować zmienne w blokach @postgres-docs:declarations.
Do tego dowiedziałem się, że w blokach nie mogę używać `SELECT` do wyświetlania wartości. Zamiast tego użyłem `RAISE NOTICE` @stackoverflow:how-to-perform-a-select-query-in-a-do-block.

Na początku nie działało mi tworzenie nowego użytkownika procedurą `add_user`. \
Przypominałem sobie, że na poprzednim etapie ustawiłem uprawnienia do tworzenia użytkowników i tylko administratorzy mogą to robić. Przełączyłem się na konto administratora i procedura zadziałała (@lst-add-user-procedure-result).

```{#lst-add-user-procedure-result .text lst-cap="Wynik procedury `add_user`"}
NOTICE: Created user: 1015
```

Zmieniłem użytkownika na _casual_gamer_ i wykonałem kolejne procedury z mojego testowego pliku [`try-simple-insert-procedures.sql`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/programmable/tests/try-simple-insert-procedures.sql). \
Zgodnie z oczekiwaniami otrzymałem wyniki @lst-other-insert-procedures-results.

```{#lst-other-insert-procedures-results .text lst-cap="Wyniki pozostałych procedur dodawania rekordów"}
NOTICE: Created location: 11
NOTICE: Created rating: 61
NOTICE: Created review: 19
NOTICE: Created game wish: 16
```

Utworzenie rekordów sprawdziłem też zapytaniami `SELECT`.

# Złożona procedura

Czasem zdarza się, że użytkownik posiada na kilka kont w systemie na przykład zalogowane mailem i założone za pośrednictwem konta dużego serwisu takiego jak na przykład Google. \
W takiej sytuacji, której sam kiedyś doświadczyłem, użytkownik po jakimś czasie może chcieć połączyć swoje konta w jedno.

W tym celu przygotowałem procedurę [`merge_user_accounts`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/programmable/complex-procedure.sql), która pozwala zmigrować dane z jednego konta do drugiego. \
Wszystkie aktualizacje danych są wykonywane w ramach transakcji. Dzięki temu baza przejdzie ze stanu spójnego do spójnego lub cofnie wszystkie zmiany.

Procedura sprawdza czy użytkownicy istnieją i czy są różni. Jeśli tak, to wykonuje następujące kroki:

1. Migruje oceny gry,
2. Migruje recenzje,
3. Migruje listy życzeń,
4. Migruje kolekcję gier,
5. Migruje lokalizacje,
6. Migruje dziennik rozegranych gier,
7. Usuwa stare konto.

Na każdym etapie sprawdzane jest czy użytkownik nie ma już danych wartości na docelowym koncie. Jeśli to możliwe, to dane są aktualizowane, a w przeciwnym razie dane starego konta są usuwane.

## Dokumentacja działania

Do przetestowania transakcji w procedurze zasymulowałem błąd pod koniec dodając `RAISE EXCEPTION`, a następnie ją wywołałem i sprawdziłem gry użytkownika (@lst-merge-user-accounts-error).

```{#lst-merge-user-accounts-error .sqlpostgresql lst-cap="Wywołanie procedury `merge_user_accounts` z dodanym błedem"}
CALL merge_user_accounts(9, 10);
SELECT * FROM user_game_release WHERE user_id = 9;
```

Po wynikach (@lst-merge-user-accounts-fail-logs) zaobserwowałem, że wystąpił błąd pod koniec działania procedury.

```{#lst-merge-user-accounts-fail-logs .txt lst-cap="Logi nieudanego łączenia kont"}
{{< include listings/user-merge-procedure-fail.txt >}}
```

Po wynikach `SELECT` (@tbl-games-after-failed-merge) widać, że dane nie zostały zmigrowane, ponieważ użytkownik nadal ma gry, Mechanizm transakcji cofnął zmiany.

| user_id | game_release_id |    acquired_at        |
|---------|-----------------|----------------------|
|   9     |      25         | 2016-02-05 14:00:00  |
|   9     |      28         | 2015-03-20 15:45:00  |
|   9     |      33         | 2019-11-01 17:15:00  |

: Gry użytkownika źródłowego po nieudanym wykonaniu procedury {#tbl-games-after-failed-merge}

Gdy usunąłem ręcznie rzucenie wyjątku, dane dane użytkownika zostały poprawnie przeniesione (@lst-merge-user-accounts-success), a zapytanie `SELECT` (@tbl-games-after-successful-merge) nie zwróciło żadnych wyników.

```{#lst-merge-user-accounts-success .txt lst-cap="Udane połączenie kont"}
{{< include listings/user-merge-procedure-success.txt >}}
```

| user_id | game_release_id |    acquired_at        |
|---------|-----------------|----------------------|

: Gry użytkownika źródłowego po poprawnym wykonaniu procedury {#tbl-games-after-successful-merge}

# Wyzwalacze

Chciałem, żeby gdy użytkownik wejdzie w posiadanie gry została ona automatycznie usunięta z jego listy życzeń. W tym celu przygotowałem wyzwalacz, który to automatyzuje.

Mój pomysł na logikę biznesową zakłada, że zapisują informację o rozgrywce (`play`) można podać większą liczbę uczestników (`player_count`) niż powiązanych graczy, ponieważ nie każdy uczestnik musi mieć konto w systemie.
Może to jednak prowadzić do błędnej sytuacji podania mniejszej liczby `player_count` niż powiązanych graczy. \
Przygotowałem wyzwalacz, który przy dodaniu relacji na większą liczbę graczy niż `player_count` aktualizuje `player_count` na większą wartość oraz drugi, który pilnuje, żeby `player_count` nie był ustawiony na mniejszą wartość niż liczba faktycznych graczy.

W PosgreSQL wyzwalacz musi wywoływać funkcję lub procedurę @postgres-docs:createtrigger, więc do każdego wyzwalacza stworzyłem też odpowiadającą procedurę. \
Kod wyzwalaczy umieściłem w pliku [`triggers.sql`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/programmable/triggers.sql).

## Dokumentacja działania

Dla użytkownika `casual_gamer` dodałem grę _Pandemic_ (@lst-add-game-wish) do jego listy życzeń @tbl-wishlist-after-pandemic-add.

```{#lst-add-game-wish .postgresql lst-cap="Dodanie gry Pandemic do listy życzeń"}
CALL add_game_wish(4, NULL);
```

|username    |name    |wished_at                 |
|------------|--------|--------------------------|
|casual_gamer|Pandemic|2025-11-07 10:39:01.453507|
|casual_gamer|Splendor|2024-09-20 14:00:00       |
|casual_gamer|Gloomhaven|2024-08-20 16:45:00       |

: Życzenia użytkownika `casual_gamer` po dodaniu gry _Pandemic_ do listy życzeń {#tbl-wishlist-after-pandemic-add}

Następnie dodałem egzemplarz gry _Pandemic_ do kolekcji użytkownika. Po dodaniu gry do kolekcji automatycznie zniknęła ona z jego listy życzeń @tbl-wishlist-after-acquire-pandemic.

|username    |name    |wished_at                 |
|------------|--------|--------------------------|
|casual_gamer|Splendor|2024-09-20 14:00:00       |
|casual_gamer|Gloomhaven|2024-08-20 16:45:00       |

: Życzenia użytkownika `casual_gamer` po dodaniu gry _Pandemic_ do kolekcji {#tbl-wishlist-after-acquire-pandemic}

Kod użyty do testów umieściłem w pliku [`test-wishlist-trigger.sql`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/programmable/tests/test-wishlist-trigger.sql).

# Inne elementy ograniczenia dostępu

Do tej pory przygotowałem już **18** elementów ograniczenia dostępu z **10 sugerowanych** w zakresie zadania.

- 5 procedur dodawania danych
- 1 złożona procedura
- 3 wyzwalacze
- 4 widoki użytkownika
- 1 widok administratora
- 4 widoki publiczne

Z tego powodu ograniczę się do zastanowienia, co jeszcze byłoby warto dodać w produkcyjnej aplikacji.

- widok statystyk użytkownika
- widok historii ceny danej edycji gry
- widok personalizowanych rekomendacji gier
- wyzwalacze pilnujące czy zwycięzca rozgrywki jest jej uczestnikiem

Sporą część zachowania spójności zapewniają już instrukcje kaskadowego usuwania powiązanych rekordów, które skonfigurowałem tworząc tabele.

# Automatyzacja zadania

## Wybór scheduler'a

Żeby uruchamiać automatycznie kod `SQL` w bazie zdecydowałem się użyć rozszerzenia [pg_cron](https://github.com/citusdata/pg_cron), o którym słyszałem w filmie _I replaced my entire tech stack with Postgres..._
 @youtube:IReplacedMyEntireTechStackWithPostgres.

Rozważałem użycie scheduler'a [pgAgent](https://www.pgadmin.org/docs/pgadmin4/9.9/pgagent.html), ale po przeczytaniu artykułu porównującego te rozwiązania @medium:comparing-postgresql-job-schedulers-pg-cron-vs-pgagent i tabeli różnić @github:pg-agent-vs-pg-cron, uznałem, że [pg_cron](https://github.com/citusdata/pg_cron) bardziej mi pasuje, ponieważ jest lżejszym rozwiązaniem i nie wymaga oddzielnego demona.
Gdyby zadanie wymagało zaawansowanej logiki uruchamiania zadań w czasie i zależności między zadaniami pewnie wybrałbym[pgAgent'a](https://www.pgadmin.org/docs/pgadmin4/9.9/pgagent.html).

## Instalacja

Żeby zainstalować rozszerzenie nie mogłem już korzystać z domyślnego obrazu Docker @docker:postgres-official-image i musiałem skonfigurować własny obraz, który bazuje na tym bazowy, na którym zainstaluję rozszerzenie.
Znalazłem obraz z od razu zainstalowanym rozszerzeniem pg_cron @docker:postgres-pg-cron-image, ale używa on starej wersji Postgres, więc wolałem stworzyć swój.

Przygotowałem własny obraz z zainstalowanym rozszerzeniem pg_cron w pliku [`Postgres.Dockerfile`](https://github.com/christopher-dabrowski/BoardBase/blob/main/Postgres.Dockerfile), zmieniłem plik [`docker-compose.yml`](https://github.com/christopher-dabrowski/BoardBase/blob/main/docker-compose.yml), żeby używał tego obrazu i skonfigurowałem \acr{SZBD} w pliku [`postgresql.conf`](https://github.com/christopher-dabrowski/BoardBase/blob/main/postgresql.conf).

Po skonfigurowaniu obrazu Docker okazało się, że nie uruchamia się kontener. Pojawiał się błąd, że w obrazach w wersji 18+ volumen musi być zamontowany poziom wyżej w kontenerze.
Zdziwiło mnie to, ponieważ wcześniej używałem po prostu obrazu `18.0`, a ten błąd nie występował. \
Przywróciłem konfigurację Docker Compose do używania bezpośrednio bazowego obrazu, ale błąd dalej występował.
Poprawiłem więc ścieżkę volumenu, co rozwiązało problem, choć dalej zastanawia mnie, dlaczego wcześniej nie miałem tego problemu. \
Przypuszczam, że został wgrany nowy obraz pod tag `18.0`, a przygotowanie mojego obrazu sprowokowało `docker pull`.

## Automatyczne zadanie

Pomyślałem, że użytkownicy często chcieliby przeglądać statystyki gier. Niektóre ze statystyk wymagają agregacji.
Ponieważ wyliczanie niektórych agregacji jest czasochłonne, a użytkownicy nie oczekują raczej w pełni aktualnych statystyk (pewnie wyniki z ostatniego dnia będą dobre), to utworzyłem zmaterializowany widok przechowujący te dane. \
Za pomocą automatycznego zadania [pg_cron](https://github.com/citusdata/pg_cron), skonfigurowanego w pliku [`automation.sql`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/automation/automation.sql), aktualizuję ten widok codziennie o 3 w nocy. \
Skonfigurowane zadanie widać w wyniku poniższego zapytania (@lst-cron-jobs) w @tbl-automation-jobs.

```{#lst-cron-jobs .sqlpostgresql lst-cap="Zadania pg_cron"}
SELECT jobid, schedule, command
FROM cron.job
```

|jobid|schedule |command                                             |
|-----|---------|----------------------------------------------------|
|1    |0 3 * * *|REFRESH MATERIALIZED VIEW main.game_popularity_stats|

: Automatyczne zadania {#tbl-automation-jobs}

\acr{SQL} widoku zapisałem w pliku [`game-stats-materialized-view.sql`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/views/game-stats-materialized-view.sql). Widok wylicza między innymi bayesowską średnią ocenę gry, którą zaimplementowałem wykorzystując moją \acr{UDF} `fair_game_rating`, liczbę gier rozegranych w ostatnich 90 dniach, czy też customowy ranking popularności, który wyliczam heurystycznie na podstawie kilku innych parametrów i inne statystyki.

# Kopia zapasowa

## Wybór podejścia
Zastanawiałem się nad wyborem między trzema głownymi podejściami do backupu @postgres-docs:backup-and-restore:

1. \acr{SQL} dump,
2. backup plików bazy danych,
3. backup plików i \acr{WAL} z odtwarzaniem bazy point in time z \acr{WAL}.

Uznałem, że podejście bazujące na \acr{WAL} jest zbyt złożone na moje potrzeby, gdy zapoznałem się z mechanizmem jego działania @postgres-docs:backup-online-wal. \
Podejście drugie wymaga zatrzymania \acr{SZBD}, co sprawia, że w produkcyjnych środowiskach jest mało praktyczne.
Dokumentacja sugeruje, żeby raczej używać podejścia bazującego na \acr{SQL} dump @postgres-docs:backup-file, więc to na nie się zdecydowałem.

## Wykonanie kopii zapasowej

Zgodnie z zaleceniami z dokumentacji @postgres-docs:backup-dump, użyłem narzędzia `pg_dump` do zrobienia backupu. \
Za pierwszym razem narzędzie `pg_dump` nie zadziałało, ponieważ okazało się, że domyślny pakiet [homebrew](https://brew.sh/) instaluje wersję 14, a mój \acr{SZBD} jest w wersji 18.
Na szczęście przyczyna błędu była dobrze opisana w komunikacie i wystarczyło zainstalować odpowiednią wersję. Zrobiłem to poleceniem `brew install postgresql@18`.

Ponieważ przeczytałem, że domyślnie `pg_dump` loguje się do bazy taką samą nazwą użytkownika @postgres-docs:backup-dump, jak zalogowane konto, to nadpisałem nazwę użytkownika i kilka innych podstawowych parametrów w wywołaniu @lst-pg-dump-backup.

```{#lst-pg-dump-backup .sh lst-cap="Wykonanie kopii zapasowej pg_dump"}
pg_dump --dbname=boardbase --username=postgres --host=localhost --file=2025-11-10_boardbase_backup.sql
```

Zgodnie z moimi oczekiwaniami powstały plik zawiera kod \acr{SQL} do odtworzenia schematu bazy, danych, a nawet utworzonego rozszerzenia.
Zawiera nawet konfigurację mojego cron joba.

## Weryfikacja kopii zapasowej

Żeby sprawdzić poprawność kopii zapasowej postanowiłem odtworzyć z niej bazę danych.

Do kontenera z \acr{SZBD} zamontowałem nowy volumen. \
Przed wgraniem kopii zapasowej utworzyłem użytkowników, ponieważ skrypt kopii nie tworzy użytkowników @postgres-docs:backup-dump.
W moim skrypcie [`1_create-groups`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/users/1_create-groups.sql) tworzę grupy i nadaję im role.
Ponieważ nie powstał jeszcze schemat `main`, to nadanie do niego ról nie zadziałało. Nie jest to problem, ponieważ przypisanie uprawnień jest zawarte w kopii zapasowej. \
Po utworzeniu grup, mogłem stworzyć użytkowników i przypisać ich do grup skryptem [`2_craete-users`](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/users/2_craete-users.sql).

Wczytałem kopię zapasową i spróbowałem uruchomić kilka zapytań. \
Okazało się, że **zapytania, w których nie podawałem jawnie nazwy schematu, nie działały**. \
Sprawdziłem, i okazało się, że w pliku backupu **nie ma mojej instrukcji zmiany domyślnego `search_path`**, żeby zawierał schemat `main`.
Wykonałem ją ręcznie. Wtedy zapytania działały już poprawnie.

# Inny \acr{SZBD}

Wybrałem \acr{SZBD} MS SQL Server.
Używałem go pośrednio w pracy w aplikacjach backendowych przez \acr{ORM} i chciałbym lepiej poznać tą technologię.

Tak jak w przypadku Postgres, zdecydowałem się na uruchomienie MS SQL Serwer w kontenerze Docker skonfigurowanym w pliku [`docker-compose.yml`](https://github.com/christopher-dabrowski/BoardBase/blob/main/docker-compose.yml). \
Przy konfiguracji kontenera wzorowałem się na oficjalnej dokumentacji @ms-learn:docker-run-containers-for-sql-server-on-linux.
Nie chciałem jednak używać obrazu _latest_, jak rekomenduje dokumentacja @ms-learn:docker-run-containers-for-sql-server-on-linux, ponieważ sądzę, że nie jest to dobra praktyka, ponieważ jest on automatycznie zmieniany. \
Na szczęście dokumentacja sugeruje alternatywnie użycie konkretnej wersji obrazu `mcr.microsoft.com/mssql/server:2025-GA-ubuntu`.
Zdziwiłem się jednak, gdy spróbowałem pobrać ten obraz, ponieważ okazało się, że **taki obraz nie istnieje**. \
Sprawdziłem to na oficjalnej liście tagów tego obrazu @microsoft-artifact-registry:mssql-server-tags.
Spojrzałem też na informację o aktualnych wersjach SQL Server @ms-learn:latest-updates-and-version-history-for-sql-server, gdzie dowiedziałem się, że wersja 2025 nie została jeszcze oficjalnie wydana, co sugerował podany w dokumentacji @ms-learn:docker-run-containers-for-sql-server-on-linux tag `2025-GA-ubuntu`, gdzie w nomenklaturze Microsoft GA oznacza publicznie wydaną wersję. \
Najwyraźniej **dokumentacja Microsoft @ms-learn:docker-run-containers-for-sql-server-on-linux podaje zmyślony tag do wersji MS SQL Server, która jeszcze nie istnieje**.

Gdy już to zrozumiałem, zdecydowałem się użyć najnowszej stabilnej wersji SQL Server 2022. \
Skonfigurowałem edycję SQL Server na developerską i zmieniłem domyślny Collation @ms-learn:collation-and-unicode-support, tak żeby wspierał kodowanie UTF-8, ponieważ wiem, że większość danych w bazie zmieści się w jednym bajcie UTF-8. \
W oddzielnym artykule dokumentacji @ms-learn:configure-and-customize-sql-server-docker-containers znalazłem informację, gdzie kontener przechowuje dane i zamontowałem tam volumen.

Działanie \acr{SZBD} sprawdziłem za łącząc się z terminala narzędziem [`sqlcmd`](https://learn.microsoft.com/en-us/sql/tools/sqlcmd/sqlcmd-utility?view=sql-server-ver17&tabs=go%2Cwindows-support&pivots=cs1-bash) i pobierając wersję zapytaniem (@lst-query-sql-server-version).

```{#lst-query-sql-server-version .sql lst-cap="Pobranie wersji MS SQL Server"}
SELECT @@VERSION;
GO;
```

Otrzymałem spodziewany wynik (@lst-query-sql-server-version-result).

```{#lst-query-sql-server-version-result .text lst-cap="Wynik zapytania pobierającego wersję MS SQL Server"}
Microsoft SQL Server 2022 (RTM-CU21-GDR) (KB5068406) - 16.0.4222.2 (X64)
	Oct  3 2025 16:55:17
	Copyright (C) 2022 Microsoft Corporation
	Developer Edition (64-bit) on Linux (Ubuntu 22.04.5 LTS) <X64>
```

# Analiza zmian między \acr{SZBD}

<!-- Zapoznaj się z różnicami pomiędzy dialektami i dokonaj analizy (którą umieść w
sprawozdaniu) jak wpłyną one na bazę danych używaną w projekcie i jakie poprawki będą
potrzebne. Proszę przygotować ten punkt sprawozdania przed wykonaniem kolejnych. Z
doświadczenia poprzedniego rocznika – nie polecam korzystania tutaj z podpowiedzi modeli
językowych (a jeśli tak to dokładnie je sprawdzić). -->

## Artykuły porównawcze

Analizę zmian zacząłem od zapoznania się z artykułem od _Google Cloud_ _PostgreSQL vs SQL Server: What are the key differences?_ @google-cloud:postgresql-vs-sql-server. Okazał się on jednak bardzo powierzchowny i prawdopodobnie mijał się miejscami z prawdą. W tabeli porównującej prezentowane jest w tym artykule, że Microsoft SQL Server "Uses Transact-SQL or T-SQL (standard SQL + extra functionality)" a PostgreSQL "Uses Standard SQL".
Z mojego doświadczenia i dokumentacji @postgres-docs:plpgsql-overview wynika, że Postgres używa własnej wersji składni `PL/pgSQL`, a nie standardowego \acr{SQL}.

Następnie przeczytałem bardziej obszerny artykuł _A Complete Comparison of PostgreSQL vs Microsoft SQL Server_ @edb:mssql-vs-postgresql.
Wydaje się on rzetelny, ale zauważyłem, że o ile są w artykule opisane rodzaje indeksów PostgreSQL, to dla \acr{MSSQL} wspomniane są tylko indeksy _clustered_ i _nonclustered_ oraz automatyczne indeksy tworzone na kluczach i _constraint_.
Sugeruje to, że \acr{MSSQL} nie ma konkretnych typów indeksów, co nie jest prawdą @ms-learn:indexes.
Podobne nieścisłości zauważyłem w sekcji opisującej wyzwalacze.

## Wpływ różnic

Typ binarny `VARBINARY` w \acr{T-SQL} wymaga podania maksymalnej długości (można podać `max`) @ms-learn:binary-and-varbinary, a używany do tej pory przeze mnie `BYTEA` w PostgreSQL tego nie wymaga @postgres-docs:binary-data-types. \
Różnią się też trochę typy do przechowywania dat @edb:mssql-vs-postgresql. \
Oznacza to, że będzie konieczna zmiana typów niektórych kolumn w bazie danych.

Odpowiednikiem automatycznego generowania sekwencyjnych wartości `GENERATED ALWAYS AS IDENTITY` z Postgres w \acr{MSSQL} jest `IDENTITY(1,1)` @ms-learn:identity.

\acr{MSSQL} ma wbudowany scheduler @edb:mssql-vs-postgresql, @ms-learn:create-a-schedule, więc nie będę musiał instalować rozszerzeń lub korzystać z zewnętrznych narzędzi.

Składnia elementów programowalnych jest nieco inna @edb:mssql-vs-postgresql. Będzie konieczna konwersja kodu do składni \acr{T-SQL}.

Przeczytałem też, że \acr{MSSQL} nie ma wsparcia dla `JSON`, które jest dostępne w Postgres @estuary:migrate-postgresql-to-sql-server.
Nie korzystałem jednak z tej funkcji.

# Migracja na inny \acr{SZBD}

## SQL Server Migration Assistant

Początkowo miałem nadzieję użyć oficjalnego narzędzia od Microsoft [SQL Server Migration Assistant](https://learn.microsoft.com/en-us/sql/ssma/sql-server-migration-assistant), ponieważ przeczytałem, że jest to możliwe w artykule @estuary:migrate-postgresql-to-sql-server.
Niestety narzędzie to wspiera tylko migrację z MySQL i Oracle @ms-learn:sql-server-migration-assistant, z \acr{SZBD} omawianych w zadaniu.

Oceniam to narzędzie jako nieprzydatne do mojej migracji.

## pg_dump

Post na _Stack Overflow_ @stackoverflow:how-to-migrate-a-postgresql-database-into-a-sqlserver
opisuje, że jednym ze sposobów na migrację jest wygenerowanie skryptów \acr{SQL} w możliwie uniwersalnej postaci do utworzenia struktury tabel i danych, a potem wczytanie ich w innym \acr{SZBD}.

Z podobnym podejściem spotkałem się w wątku na _Server Fault_ @serverfault:best-tool-to-migrate-a-postgresql-database-to-ms-sql-2005.
Autor opisywał w nim też transformację formatu wyeksportowanych danych skryptem i wczytanie ich narzędziem [`bcp`](https://learn.microsoft.com/en-us/sql/tools/bcp-utility).
Do przeniesienia schematu bazy sugerował narzędzie [Full Convert](https://www.fullconvert.com/).

Sądzę, że to podejście nie przeniesie żadnych elementów programowalnych, a jedynie dane i schemat, co i tak może wymagać ręcznych poprawek. \
Chciałbym spróbować bardziej automatycznego narzędzia, ale możliwe, że wrócę do tego podejścia, ponieważ mimo swoich ograniczeń wydaje się mało zawodne.

## DBConvert

Narzędzie [DBConvert](https://dbconvert.com/postgresql/mssql/) automatycznie migruje tabele, indeksy i widoki oraz dane z bazy PostgreSQL do \acr{MSSQL}. \
Dokonywane jest mapowanie typów, a w przypadku braku jednoznacznego dopasowania narzędzie pozwala na ręczne dobranie docelowych typów. \
Wygląda na to, że nie wykonuje migracji elementów programowalnych @dbconvert:postgresql-to-sql-server. \
Narzędzie to jest dostępne tylko w wersji na system Windows.

Migracja danych i schematu z narzędziem [DBConvert](https://dbconvert.com/postgresql/mssql/) wydaje się łatwiejsza niż za pomocą `pg_dump`, jednak chciałbym użyć narzędzia, które zmigruje też automatyzację bazy. Niedostępność na systemie MacOS sprawia, że nie mogę bezpośrednio uruchomić tego narzędzia.

## Sqlines {#sec-sqlines}

Narzędzie [Sqlines](https://www.sqlines.com/postgresql-to-sql-server) składa się z dwóch modułów i wspiera migrację z PostgreSQL do \acr{MSSQL} @sqlines:postgresql-to-sql-server. \
Moduł _SQLines Data_ realizuje migrację danych i schematu.
Moduł _SQLines SQL Converter_ pozwala przekonwertować skrypt \acr{SQL} do formatu docelowej bazy (\acr{T-SQL}). Możliwa jest też konwersja pojedynczych plików w [wersji online](https://www.sqlines.com/online).

Wersja próbna narzędzia wprowadza nietypowe ograniczenia. \
Moduł _SQLines Data_ może nie zaimportować jednego wiersza z oryginalnej tabeli, a moduł _SQLines SQL Converter_ może dodawać lub usuwać komentarze @sqlines:download.

Niestety najnowsza wspierana wersja bazy PostgreSQL to 16.x @sqlines:postgresql-to-sql-server, a niestety moja baza jest w wersji 18.0. \
Narzędzie to jest dostępne tylko w wersji na system Windows. \
Mimo tych potencjalnych ograniczeń chciałbym je wypróbować z uwagi na wsparcie konwersji skryptów \acr{SQL}. Nie spotkałem się jeszcze z innym narzędziem, które by to robiło.

## DBeaver

[DBeaver](https://dbeaver.io/) to uniwersalny program to pracy z bazami danych, który wspiera też migrację danych @dbeaver:data-migration. \
Dostępna jest darmowa wersja nawet na MacOS.

Ponieważ brak importu pojedynczych wierszy [Sqlines](https://www.sqlines.com/postgresql-to-sql-server) (@sec-sqlines), będzie kłopotliwy przy kluczach obcych, chciałbym użyć [DBeaver](https://dbeaver.io/) do importu danych, a [Sqlines](https://www.sqlines.com/postgresql-to-sql-server) do dostosowania elementów programowalnych.

## Ispirer

Firma [Ispirer](https://www.ispirer.com/) oferuje narzędzia do migracji baz danych. Niestety nie wspierają PostgreSQL jako źródła, a jedynie jako docelową bazę @ispirer:requesting-license.

# Migracja bazy

Zdecydowałem się na hybrydowe podejście, w którym użyję [DBeaver](https://dbeaver.io/) do importu danych i [Sqlines](https://www.sqlines.com/postgresql-to-sql-server) do dostosowania elementów programowalnych. \
Wybrałem [DBeaver](https://dbeaver.io/) ponieważ jest dostępny na MacOS i posiada darmową wersję. \
[Sqlines](https://www.sqlines.com/postgresql-to-sql-server) nie jest dostępny na MacOS, ale ponieważ operuje na plikach kodu \acr{SQL} to stosunkowo łatwo będzie mi skorzystać z tego narzędzia na innym komputerze. Niestety narzędzie to wspiera tylko wersje PostgreSQL do 16.x, ale liczę na to, że mój kod działający na PostgreSQL 18.0 będzie kompatybilny.

Myślałem nad zastosowaniem wyłącznie obu narzędzi [Sqlines](https://www.sqlines.com/postgresql-to-sql-server), jednak obawiałem się ograniczenia wersji próbnej, która może nie zaimportować po jednym wierszu z każdej tabeli.
Spodziewam się, że spowodowałoby to problemy z kluczami obcymi, które musiałbym ręcznie naprawiać. \
Musiałbym też przenieść dane bazy na Windows. Spodziewam się, że byłoby to możliwe za pomocą eksportu i importu volumenów Docker @docker:volumes, ale wolę tego uniknąć.

## Dane

### DBeaver

[DBeaver](https://dbeaver.io/) wymaga podania istniejącej bazy i schemat, do której zostaną zmigrowane wybrane tabele.
W tym celu utworzyłem nową bazę _boardbase_ na \acr{MSSQL} i schemat `main`. \
Do utworzenia schematu spróbowałem przekonwertować skrypt [1_create-schema.sql](https://github.com/christopher-dabrowski/BoardBase/blob/main/sql/schema/1_create-schema.sql) [Sqlines online](https://www.sqlines.com/online). Niestety narzędzie zmieniło tylko komentarz. Kod \acr{SQL} pozostał bez zmian (poza dodaniem `GO`) i nie działał na \acr{MSSQL}. Poprawiłem go ręcznie.

Dowiedziałem się, że w \acr{MSSQL} nie ma konceptu `search_path` @dba-stackexchange:does-t-sql-have-a-schema-search-path,
ale można skonfigurować domyślny schemat dla danego użytkownika @ms-learn:alter-user. Niestety nie udało mi się tego zrobić dla użytkownika `sa`.

Po uruchomieniu migracji napotkałem błąd przy tworzeniu tabeli `game_wish`.

> SQL Error [4188] [S0001]: Column or parameter 'want_level' has type 'text' and collation 'Latin1_General_100_CI_AS_SC_UTF8'. The legacy LOB types do not support UTF-8 or UTF-16 encodings. Use types varchar(max), nvarchar(max) or a collation which does not have the _SC or _UTF8 flags.
> Column or parameter 'want_level' has type 'text' and collation 'Latin1_General_100_CI_AS_SC_UTF8'. The legacy LOB types do not support UTF-8 or UTF-16 encodings. Use types varchar(max), nvarchar(max) or a collation which does not have the _SC or _UTF8 flags.

Zauważyłem, że [DBeaver](https://dbeaver.io/) zmienił typ kolumny `want_level` na `text`.
Oryginalnie używałem własnego typu `ENUM`. `ENUM` nie jest wspierany w \acr{MSSQL} @bigdatansql:enum-equivalent-in-sql-server. \
Zmieniłem domyślne mapowanie typu kolumny `want_level` na `nvarchar(20)`.

Po tej poprawie dane zostały poprawnie zaimportowane do \acr{MSSQL}.
Sprawdziłem, że tabela `game_wish` zawiera poprawne informacje w kolumnie `want_level`, teraz zapisane jako zwykły tekst. \
Sprawdziłem też, czy zostały przeniesione moje `CONSTRAINT` dbające na przykład o poprawność formatu maili czy niedopuszczanie pustych wartości w niektórych kolumnach. Niestety żadne z nich nie zostały przeniesione, nawet tak podstawowe jak `UNIQUE`. \
Nie przeniosły się też klucze obce.

Nie jest to zadowalający mnie wynik. Chciałbym wypróbować narzędzi działających na systemie Windows.
W tym celu wyeksportowałem Docer'owy volumen z danymi i przeniosłem go komputer z systemem Windows.

### DBConvert

Po wczytaniu volumenu z danymi i odpaleniu baz na Widowsie uruchomiłem narzędzie [DBConvert](https://dbconvert.com/postgresql/mssql/).

Nowy typ dla kolumny `want_level` narzędzie ustawiło domyślnie na VARCHAR(800), co zadziałałoby lepiej niż `TEXT`, ale i tak poprawiłem typ ręcznie na `VARCHAR(20)`. \
Domyślnie narzędzie [DBConvert](https://dbconvert.com/postgresql/mssql/) pokazywało problemy z kluczami obcymi (@fig-dbconvert-type-issues).
Klucze główne miały ustawiony typ `BIGINT`, a klucze obce `INT`. Rozwiązałem to zmieniając typy kluczy obcych na `BIGINT`.

::: {#fig-dbconvert-type-config layout="[50, -5, 50]"}

![Domyślne ustawienie typów](img/MSSQL2PostgreSQLPro_initial-conversion-issues.png){#fig-dbconvert-type-issues}

![Poprawione klucze obce](img/MSSQL2PostgreSQLPro_fixed-types.png)

Konfiguracja importu danych w DBConvert
:::

Niestety po uruchomieniu migracji kończyła się ona błędami (@fig-dbconvert-import-failure).

![Błędy migracji DBConvert](img/MSSQL2PostgreSQLPro_import-failure.png){#fig-dbconvert-import-failure}

### Full Convert

Chciałem wypróbować jeszcze narzędzie [Full Convert](https://www.fullconvert.com).
Jest ono dostępne tylko na Windows, ale na to byłem już przygotowany. \

Podczas instalacji Windows Defender oznaczył jeden z plików jako trojan, co spowodowało niepowodzenie instalacji (@fig-full-convert-install-issues).
Wolałem nie ryzykować i zrezygnowałem z użycia tego narzędzia.

::: {#fig-full-convert-install-issues layout="[50, -5, 50]"}

![Problem wykryty przez Windows Defender](img/WindowsDefender_full-converter-trojan-error.png)

![Instalacja zakończona niepowodzeniem](img/SetupFullConvert_installation-error.png)

Problemy z instalacją Full Convert
:::

# Źródła {.unnumbered}

::: {#refs}
:::
